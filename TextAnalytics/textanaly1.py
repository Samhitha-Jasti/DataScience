# -*- coding: utf-8 -*-
"""TextAnaly1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zen1h2ly517z9HEwTFdksGymM_CPfYEC
"""

import nltk
from nltk import sent_tokenize,word_tokenize
nltk.download('punkt')

text= "Hello iam doing text analytics here"
a=sent_tokenize(text)
b=word_tokenize(text)
a

b

# Frequency Distribution
from nltk import FreqDist
f=FreqDist(b)
f

# stop words
from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words=list(stopwords.words('english'))
stop_words

# all the stop words will be ommited from a
new_list=[]
for i in a:
  if i not in stop_words:
    new_list.append(i)
    print(new_list)

# Lexical Normalization
# stemming and lemitization
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

porter_stem=PorterStemmer()
new_text=[]
for i in a:
  if i not in stop_words:
    new_text.append(i)
    print(new_text)

# lemitization 
lemma=WordNetLemmatizer()
nltk.download('wordnet')

word="trying"
a=lemma.lemmatize(word,'v')
b=porter_stem.stem(word)
a

b

nltk.download('averaged_perceptron_tagger')

sentence="hello, iam new to nlp, nice to see you"
token=word_tokenize(sentence)
nltk.pos_tag(token)

